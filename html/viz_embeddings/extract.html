<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>viz_embeddings.extract API documentation</title>
<meta name="description" content="Extractor for TensorFlow Embbeding Projector â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>viz_embeddings.extract</code></h1>
</header>
<section id="section-intro">
<p>Extractor for TensorFlow Embbeding Projector
</p>
<p>Embbeding are extracted from text using HuggingFace Transformers.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Extractor for TensorFlow Embbeding Projector  

Embbeding are extracted from text using HuggingFace Transformers.
&#34;&#34;&#34;

from enum import Enum
import math

from transformers import AutoModel, AutoTokenizer
import pandas as pd
import nltk
import torch

class Level(Enum):
    &#34;&#34;&#34; Input text segmentation levels
    &#34;&#34;&#34;

    token = 1
    word = 2
    sentence = 3

def form_token_vocab(tokenizer, text):
    &#34;&#34;&#34;Separates text in tokens  

        Args:  
            text (str): input text  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  

       Returns:  
          tokens (list) : tokens list
    &#34;&#34;&#34;

    tokens = tokenizer.tokenize(text)
    return tokens

def form_word_vocab(text):
    &#34;&#34;&#34;Separates text in words  

        Args:  
            text (str): input text  

       Returns:  
          words (list) : words list
    &#34;&#34;&#34;

    nltk.download(&#39;punkt&#39;)

    words = nltk.word_tokenize(text)
    return words

def form_sentence_vocab(text):
    &#34;&#34;&#34;Separates text in sentences  

        Args:  
            text (str): input text  

       Returns:  
          sentences (list) : sentences list
    &#34;&#34;&#34;

    nltk.download(&#39;punkt&#39;)

    sentences = nltk.sent_tokenize(text)
    return sentences

def form_token_embeddings(text, tokenizer, model):
    &#34;&#34;&#34;Generate tokens embeddings list 

        Args:  
            text (str): input text  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  

       Returns:  
            embeddings (torch.tensor): tokens embeddings tensor
    &#34;&#34;&#34;
    encoded = tokenizer.encode(text, add_special_tokens=False)
    input_ids = torch.tensor(encoded).unsqueeze(0)
    embeddings = model(input_ids)[0][0]

    return embeddings


def form_word_embeddings(text, words, tokenizer, model, aggr_func):
    &#34;&#34;&#34;Generate tokens embeddings list  

        Words composed by more than one token, have tokens embbedings 
        combined in one embbeding through aggr_func.

        Args:  
            text (str): input text  
            words (str): word list  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  
            aggr_func (func): Agreggation Function to form embedding  

       Returns:  
            embeddings (torch.tensor): tokens embeddings tensor
    &#34;&#34;&#34;

    tokens = form_token_vocab(tokenizer, text)
    tokens_tensor = form_token_embeddings(text, tokenizer, model)

    token_idx = 0
    map_idx = list()
    for idx, word in enumerate(words):
        token = tokens[token_idx]

        if word == tokens[token_idx]:
            map_idx.append(idx)
        else:
            word_idx = list()
            while token != word:
                token_idx = token_idx+1
                token += tokens[token_idx].replace(&#39;#&#39;, &#34;&#34;)
                word_idx.append(token_idx)
            map_idx.append(word_idx)
        token_idx = token_idx+1

    word_embeddings = None
    for mapping in map_idx:
        if isinstance(mapping, int):
            word_embedding = tokens_tensor[mapping].unsqueeze(0)
        elif isinstance(mapping, list):
            #[tokens_tensor[idx] for idx in mapping] for tensor#
            word_embedding = None
            for idx in mapping: 
                token_tensor = tokens_tensor[idx].unsqueeze(0)
                word_embedding = token_tensor if word_embedding == None \
                                 else torch.cat((word_embedding, token_tensor), 0)
            
            word_embedding = aggr_func(word_embedding, 0).unsqueeze(0)
        else:
            raise TypeError(&#39;mapping should be list or int&#39;)
        word_embeddings = word_embedding if word_embeddings == None \
                          else torch.cat((word_embeddings, word_embedding), 0) 
    return word_embeddings

def form_sentence_embeddings(sentences, tokenizer, model, aggr_func):
    &#34;&#34;&#34;Generate sentence embeddings list  

        Sentences composed by more than one token, have tokens embbedings 
        combined in one embbeding through aggr_func.  

        Args:  
            sentences (str): sentences list  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  
            aggr_func (func): function to aggregate tensor embbedings  
            
       Returns:  
            embeddings (torch.tensor): tokens embeddings tensor
    &#34;&#34;&#34;
    sentence_embeddings = None
    for sentence in sentences:
        token_embeddings = form_token_embeddings(sentence, tokenizer, model)
        sentence_embedding = aggr_func(token_embeddings, 0).unsqueeze(0)
        sentence_embeddings =  sentence_embedding if sentence_embeddings == None \
                            else torch.cat((sentence_embeddings, sentence_embedding), 0) 


    return  sentence_embeddings

def vocabulary_and_embeddings(text, tokenizer, model, level, aggr_func):
    &#34;&#34;&#34;Generate embeddings vector and vocabulary  

        Vocabulary is chopped according to level.
        If needed, aggregation function is used.

        Args:  
            text (str): input text  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  
            level (str): Segmentation level  
            aggr_func (func): function to aggregate tensor embbedings  

       Returns:  
            vocab (list): embeddings vocabulary
            embeddings (list): embeddings
    &#34;&#34;&#34;

    if level == &#39;token&#39;:
        vocab = form_token_vocab(tokenizer, text)
        embeddings = form_token_embeddings(text, tokenizer, model)
    elif level == &#39;word&#39;:
        vocab = form_word_vocab(text)
        embeddings = form_word_embeddings(text, vocab, tokenizer, model, aggr_func)
    elif level == &#39;sentence&#39;:
        vocab = form_sentence_vocab(text)
        embeddings = form_sentence_embeddings(vocab, tokenizer, model, aggr_func)

    return vocab, embeddings

def filter_vocabulary(vocab, embeddings, filter_func):
    &#34;&#34;&#34;Filter vocabulary according to filter_func

        Args:  
            vocab(list): embeddings vocabulary  
            embeddings(list): embeddings vector  
            filter_func (func): Filter vocabulary level function  

       Returns:  
            vocab(list): filtered vocabulary  
            embeddings(list): filtered embeddings vocabulary  
    &#34;&#34;&#34;
    vocab_idx = [idx for idx in range(len(vocab)) if filter_func(vocab[idx])]
    selected_vocab = [vocab[idx] for idx in vocab_idx]
    
    #embeddings = [embeddings[idx] for idx in vocab_idx]
    selected_embedding = None
    for idx in vocab_idx:
        selected = embeddings[idx].unsqueeze(0)

        selected_embedding = selected if selected_embedding == None \
                             else torch.cat((selected_embedding, selected), 0)

    return selected_vocab, selected_embedding

def unique_vocabulary(vocab, embeddings, aggr_func, do_lower):
    &#34;&#34;&#34;Combined repeated terms into a unique term  
    
      Depends on segmentation level on vocab.
      If vocab is chopped in words, words will be considered.
      Hence, it is probably useless in sentences segmentation.

        Args:  
            vocab(list): embeddings vocabulary  
            embeddings(list): embeddings vector  
            aggr_func (func): function to aggregate tensor embbedings  
             do_lower (bool): whether considere cased  

       Returns:  
            vocab(list): embeddings vocabulary with unique values  
            embeddings(list): vocabulary embeddings  
    &#34;&#34;&#34;
    unique_vocab = list(set([term.lower() for term in vocab])) if do_lower\
                    else list(set(vocab)) 
    

    compare = lambda x, y: x.lower() == y.lower() if do_lower \
              else x == y

    unique_embeddings = None
    for unique_term in unique_vocab:

        unique_embedding = None
        for idx in range(len(vocab)):
            if compare(vocab[idx],unique_term):
                selected = embeddings[idx].unsqueeze(0)              

                unique_embedding = selected if unique_embedding == None \
                                   else  torch.cat((unique_embedding, selected), 0)

        unique_embedding = aggr_func(unique_embedding, 0).unsqueeze(0)

        unique_embeddings = unique_embedding if unique_embeddings == None \
                            else torch.cat((unique_embeddings, unique_embedding), 0)
    
    return unique_vocab, unique_embeddings

def extract_embeddings(text, model_name, level_name, embeddings_file, vocab_file, aggr_func=torch.mean, filter_func=lambda x: len(x) &gt;= 3, unique = True, doc_stride=1, do_lower=False):
    &#34;&#34;&#34;Extract embeddings from text using Hugging Face model  
            
        Args:  
            text (str): input text  
            model_name (str): Hugging Face model name  
            level_name (str): text segmentation level name
            embeddings_file (str): file to save embeddings
            vocab_file (str): file to save vocabulary
            aggr_func (func): function to aggregate tensor embbedings  
            filter_func (func): filter vocabulary level function  
            unique (bool): if terms on level_name should be unique  
            do_lower (bool): when unique = True, whether considere cased


       Raises:  
            ValueError: f&#39;There is no {level_name} segmentation.&#39;  
    &#34;&#34;&#34;

    if level_name not in Level._member_names_:
        raise ValueError(f&#39;There is no {level_name} segmentation.&#39;)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
  
    words =  form_word_vocab(text)
    doc_length = math.ceil(len(words)/doc_stride)
    doc_lst = [words[k*doc_length: min((k+1)*doc_length, len(words))]\
                for k in range(doc_stride)]
    doc_lst = [&#34; &#34;.join(doc) for doc in doc_lst]
    vocab = list()
    embeddings = list()

    embeddings = None
    try:
        for doc in doc_lst:
            doc_vocab, doc_embeddings = vocabulary_and_embeddings(doc, 
                                                                tokenizer, 
                                                                model, 
                                                                level_name, 
                                                                aggr_func)
            vocab.extend(doc_vocab)
            embeddings = doc_embeddings if embeddings == None \
                        else torch.cat((embeddings, doc_embeddings), 0)
    except RuntimeError:
        message = f&#39;Please use doc_stride higher than {doc_stride} for this input&#39;
        print(message)
        return 

    if unique:
        vocab, embeddings = unique_vocabulary(vocab, 
                                              embeddings,
                                              aggr_func,
                                              do_lower)

    if filter_func:
        vocab, embeddings = filter_vocabulary(vocab, 
                                              embeddings,
                                              filter_func)


    embeddings = embeddings.tolist()

    df = pd.DataFrame(embeddings)
    series = pd.Series(vocab)
    assert df.shape[0] == series.size

    df.to_csv(embeddings_file, index=None, sep=&#39;\t&#39;, header=None)
    series.to_csv(vocab_file, index=None, sep=&#39;\n&#39;, header=None)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="viz_embeddings.extract.extract_embeddings"><code class="name flex">
<span>def <span class="ident">extract_embeddings</span></span>(<span>text, model_name, level_name, embeddings_file, vocab_file, aggr_func=&lt;built-in method mean of type object&gt;, filter_func=&lt;function &lt;lambda&gt;&gt;, unique=True, doc_stride=1, do_lower=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract embeddings from text using Hugging Face model
</p>
<p>Args:<br>
text (str): input text<br>
model_name (str): Hugging Face model name<br>
level_name (str): text segmentation level name
embeddings_file (str): file to save embeddings
vocab_file (str): file to save vocabulary
aggr_func (func): function to aggregate tensor embbedings<br>
filter_func (func): filter vocabulary level function<br>
unique (bool): if terms on level_name should be unique<br>
do_lower (bool): when unique = True, whether considere cased</p>
<p>Raises:<br>
ValueError: f'There is no {level_name} segmentation.'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_embeddings(text, model_name, level_name, embeddings_file, vocab_file, aggr_func=torch.mean, filter_func=lambda x: len(x) &gt;= 3, unique = True, doc_stride=1, do_lower=False):
    &#34;&#34;&#34;Extract embeddings from text using Hugging Face model  
            
        Args:  
            text (str): input text  
            model_name (str): Hugging Face model name  
            level_name (str): text segmentation level name
            embeddings_file (str): file to save embeddings
            vocab_file (str): file to save vocabulary
            aggr_func (func): function to aggregate tensor embbedings  
            filter_func (func): filter vocabulary level function  
            unique (bool): if terms on level_name should be unique  
            do_lower (bool): when unique = True, whether considere cased


       Raises:  
            ValueError: f&#39;There is no {level_name} segmentation.&#39;  
    &#34;&#34;&#34;

    if level_name not in Level._member_names_:
        raise ValueError(f&#39;There is no {level_name} segmentation.&#39;)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
  
    words =  form_word_vocab(text)
    doc_length = math.ceil(len(words)/doc_stride)
    doc_lst = [words[k*doc_length: min((k+1)*doc_length, len(words))]\
                for k in range(doc_stride)]
    doc_lst = [&#34; &#34;.join(doc) for doc in doc_lst]
    vocab = list()
    embeddings = list()

    embeddings = None
    try:
        for doc in doc_lst:
            doc_vocab, doc_embeddings = vocabulary_and_embeddings(doc, 
                                                                tokenizer, 
                                                                model, 
                                                                level_name, 
                                                                aggr_func)
            vocab.extend(doc_vocab)
            embeddings = doc_embeddings if embeddings == None \
                        else torch.cat((embeddings, doc_embeddings), 0)
    except RuntimeError:
        message = f&#39;Please use doc_stride higher than {doc_stride} for this input&#39;
        print(message)
        return 

    if unique:
        vocab, embeddings = unique_vocabulary(vocab, 
                                              embeddings,
                                              aggr_func,
                                              do_lower)

    if filter_func:
        vocab, embeddings = filter_vocabulary(vocab, 
                                              embeddings,
                                              filter_func)


    embeddings = embeddings.tolist()

    df = pd.DataFrame(embeddings)
    series = pd.Series(vocab)
    assert df.shape[0] == series.size

    df.to_csv(embeddings_file, index=None, sep=&#39;\t&#39;, header=None)
    series.to_csv(vocab_file, index=None, sep=&#39;\n&#39;, header=None)</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.filter_vocabulary"><code class="name flex">
<span>def <span class="ident">filter_vocabulary</span></span>(<span>vocab, embeddings, filter_func)</span>
</code></dt>
<dd>
<div class="desc"><p>Filter vocabulary according to filter_func</p>
<p>Args:<br>
vocab(list): embeddings vocabulary<br>
embeddings(list): embeddings vector<br>
filter_func (func): Filter vocabulary level function
</p>
<p>Returns:<br>
vocab(list): filtered vocabulary<br>
embeddings(list): filtered embeddings vocabulary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_vocabulary(vocab, embeddings, filter_func):
    &#34;&#34;&#34;Filter vocabulary according to filter_func

        Args:  
            vocab(list): embeddings vocabulary  
            embeddings(list): embeddings vector  
            filter_func (func): Filter vocabulary level function  

       Returns:  
            vocab(list): filtered vocabulary  
            embeddings(list): filtered embeddings vocabulary  
    &#34;&#34;&#34;
    vocab_idx = [idx for idx in range(len(vocab)) if filter_func(vocab[idx])]
    selected_vocab = [vocab[idx] for idx in vocab_idx]
    
    #embeddings = [embeddings[idx] for idx in vocab_idx]
    selected_embedding = None
    for idx in vocab_idx:
        selected = embeddings[idx].unsqueeze(0)

        selected_embedding = selected if selected_embedding == None \
                             else torch.cat((selected_embedding, selected), 0)

    return selected_vocab, selected_embedding</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.form_sentence_embeddings"><code class="name flex">
<span>def <span class="ident">form_sentence_embeddings</span></span>(<span>sentences, tokenizer, model, aggr_func)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate sentence embeddings list
</p>
<p>Sentences composed by more than one token, have tokens embbedings
combined in one embbeding through aggr_func.
</p>
<p>Args:<br>
sentences (str): sentences list<br>
tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer<br>
model (transformers.AutoModel): HuggingFace Transformer Model<br>
aggr_func (func): function to aggregate tensor embbedings
</p>
<p>Returns:<br>
embeddings (torch.tensor): tokens embeddings tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def form_sentence_embeddings(sentences, tokenizer, model, aggr_func):
    &#34;&#34;&#34;Generate sentence embeddings list  

        Sentences composed by more than one token, have tokens embbedings 
        combined in one embbeding through aggr_func.  

        Args:  
            sentences (str): sentences list  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  
            aggr_func (func): function to aggregate tensor embbedings  
            
       Returns:  
            embeddings (torch.tensor): tokens embeddings tensor
    &#34;&#34;&#34;
    sentence_embeddings = None
    for sentence in sentences:
        token_embeddings = form_token_embeddings(sentence, tokenizer, model)
        sentence_embedding = aggr_func(token_embeddings, 0).unsqueeze(0)
        sentence_embeddings =  sentence_embedding if sentence_embeddings == None \
                            else torch.cat((sentence_embeddings, sentence_embedding), 0) 


    return  sentence_embeddings</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.form_sentence_vocab"><code class="name flex">
<span>def <span class="ident">form_sentence_vocab</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>Separates text in sentences
</p>
<p>Args:<br>
text (str): input text
</p>
<p>Returns:<br>
sentences (list) : sentences list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def form_sentence_vocab(text):
    &#34;&#34;&#34;Separates text in sentences  

        Args:  
            text (str): input text  

       Returns:  
          sentences (list) : sentences list
    &#34;&#34;&#34;

    nltk.download(&#39;punkt&#39;)

    sentences = nltk.sent_tokenize(text)
    return sentences</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.form_token_embeddings"><code class="name flex">
<span>def <span class="ident">form_token_embeddings</span></span>(<span>text, tokenizer, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate tokens embeddings list </p>
<p>Args:<br>
text (str): input text<br>
tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer<br>
model (transformers.AutoModel): HuggingFace Transformer Model
</p>
<p>Returns:<br>
embeddings (torch.tensor): tokens embeddings tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def form_token_embeddings(text, tokenizer, model):
    &#34;&#34;&#34;Generate tokens embeddings list 

        Args:  
            text (str): input text  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  

       Returns:  
            embeddings (torch.tensor): tokens embeddings tensor
    &#34;&#34;&#34;
    encoded = tokenizer.encode(text, add_special_tokens=False)
    input_ids = torch.tensor(encoded).unsqueeze(0)
    embeddings = model(input_ids)[0][0]

    return embeddings</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.form_token_vocab"><code class="name flex">
<span>def <span class="ident">form_token_vocab</span></span>(<span>tokenizer, text)</span>
</code></dt>
<dd>
<div class="desc"><p>Separates text in tokens
</p>
<p>Args:<br>
text (str): input text<br>
tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer
</p>
<p>Returns:<br>
tokens (list) : tokens list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def form_token_vocab(tokenizer, text):
    &#34;&#34;&#34;Separates text in tokens  

        Args:  
            text (str): input text  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  

       Returns:  
          tokens (list) : tokens list
    &#34;&#34;&#34;

    tokens = tokenizer.tokenize(text)
    return tokens</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.form_word_embeddings"><code class="name flex">
<span>def <span class="ident">form_word_embeddings</span></span>(<span>text, words, tokenizer, model, aggr_func)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate tokens embeddings list
</p>
<p>Words composed by more than one token, have tokens embbedings
combined in one embbeding through aggr_func.</p>
<p>Args:<br>
text (str): input text<br>
words (str): word list<br>
tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer<br>
model (transformers.AutoModel): HuggingFace Transformer Model<br>
aggr_func (func): Agreggation Function to form embedding
</p>
<p>Returns:<br>
embeddings (torch.tensor): tokens embeddings tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def form_word_embeddings(text, words, tokenizer, model, aggr_func):
    &#34;&#34;&#34;Generate tokens embeddings list  

        Words composed by more than one token, have tokens embbedings 
        combined in one embbeding through aggr_func.

        Args:  
            text (str): input text  
            words (str): word list  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  
            aggr_func (func): Agreggation Function to form embedding  

       Returns:  
            embeddings (torch.tensor): tokens embeddings tensor
    &#34;&#34;&#34;

    tokens = form_token_vocab(tokenizer, text)
    tokens_tensor = form_token_embeddings(text, tokenizer, model)

    token_idx = 0
    map_idx = list()
    for idx, word in enumerate(words):
        token = tokens[token_idx]

        if word == tokens[token_idx]:
            map_idx.append(idx)
        else:
            word_idx = list()
            while token != word:
                token_idx = token_idx+1
                token += tokens[token_idx].replace(&#39;#&#39;, &#34;&#34;)
                word_idx.append(token_idx)
            map_idx.append(word_idx)
        token_idx = token_idx+1

    word_embeddings = None
    for mapping in map_idx:
        if isinstance(mapping, int):
            word_embedding = tokens_tensor[mapping].unsqueeze(0)
        elif isinstance(mapping, list):
            #[tokens_tensor[idx] for idx in mapping] for tensor#
            word_embedding = None
            for idx in mapping: 
                token_tensor = tokens_tensor[idx].unsqueeze(0)
                word_embedding = token_tensor if word_embedding == None \
                                 else torch.cat((word_embedding, token_tensor), 0)
            
            word_embedding = aggr_func(word_embedding, 0).unsqueeze(0)
        else:
            raise TypeError(&#39;mapping should be list or int&#39;)
        word_embeddings = word_embedding if word_embeddings == None \
                          else torch.cat((word_embeddings, word_embedding), 0) 
    return word_embeddings</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.form_word_vocab"><code class="name flex">
<span>def <span class="ident">form_word_vocab</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>Separates text in words
</p>
<p>Args:<br>
text (str): input text
</p>
<p>Returns:<br>
words (list) : words list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def form_word_vocab(text):
    &#34;&#34;&#34;Separates text in words  

        Args:  
            text (str): input text  

       Returns:  
          words (list) : words list
    &#34;&#34;&#34;

    nltk.download(&#39;punkt&#39;)

    words = nltk.word_tokenize(text)
    return words</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.unique_vocabulary"><code class="name flex">
<span>def <span class="ident">unique_vocabulary</span></span>(<span>vocab, embeddings, aggr_func, do_lower)</span>
</code></dt>
<dd>
<div class="desc"><p>Combined repeated terms into a unique term
</p>
<p>Depends on segmentation level on vocab.
If vocab is chopped in words, words will be considered.
Hence, it is probably useless in sentences segmentation.</p>
<p>Args:<br>
vocab(list): embeddings vocabulary<br>
embeddings(list): embeddings vector<br>
aggr_func (func): function to aggregate tensor embbedings<br>
do_lower (bool): whether considere cased
</p>
<p>Returns:<br>
vocab(list): embeddings vocabulary with unique values<br>
embeddings(list): vocabulary embeddings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unique_vocabulary(vocab, embeddings, aggr_func, do_lower):
    &#34;&#34;&#34;Combined repeated terms into a unique term  
    
      Depends on segmentation level on vocab.
      If vocab is chopped in words, words will be considered.
      Hence, it is probably useless in sentences segmentation.

        Args:  
            vocab(list): embeddings vocabulary  
            embeddings(list): embeddings vector  
            aggr_func (func): function to aggregate tensor embbedings  
             do_lower (bool): whether considere cased  

       Returns:  
            vocab(list): embeddings vocabulary with unique values  
            embeddings(list): vocabulary embeddings  
    &#34;&#34;&#34;
    unique_vocab = list(set([term.lower() for term in vocab])) if do_lower\
                    else list(set(vocab)) 
    

    compare = lambda x, y: x.lower() == y.lower() if do_lower \
              else x == y

    unique_embeddings = None
    for unique_term in unique_vocab:

        unique_embedding = None
        for idx in range(len(vocab)):
            if compare(vocab[idx],unique_term):
                selected = embeddings[idx].unsqueeze(0)              

                unique_embedding = selected if unique_embedding == None \
                                   else  torch.cat((unique_embedding, selected), 0)

        unique_embedding = aggr_func(unique_embedding, 0).unsqueeze(0)

        unique_embeddings = unique_embedding if unique_embeddings == None \
                            else torch.cat((unique_embeddings, unique_embedding), 0)
    
    return unique_vocab, unique_embeddings</code></pre>
</details>
</dd>
<dt id="viz_embeddings.extract.vocabulary_and_embeddings"><code class="name flex">
<span>def <span class="ident">vocabulary_and_embeddings</span></span>(<span>text, tokenizer, model, level, aggr_func)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate embeddings vector and vocabulary
</p>
<p>Vocabulary is chopped according to level.
If needed, aggregation function is used.</p>
<p>Args:<br>
text (str): input text<br>
tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer<br>
model (transformers.AutoModel): HuggingFace Transformer Model<br>
level (str): Segmentation level<br>
aggr_func (func): function to aggregate tensor embbedings
</p>
<p>Returns:<br>
vocab (list): embeddings vocabulary
embeddings (list): embeddings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vocabulary_and_embeddings(text, tokenizer, model, level, aggr_func):
    &#34;&#34;&#34;Generate embeddings vector and vocabulary  

        Vocabulary is chopped according to level.
        If needed, aggregation function is used.

        Args:  
            text (str): input text  
            tokenizer (transformers.AutoTokenizer): HuggingFace Tokenizer  
            model (transformers.AutoModel): HuggingFace Transformer Model  
            level (str): Segmentation level  
            aggr_func (func): function to aggregate tensor embbedings  

       Returns:  
            vocab (list): embeddings vocabulary
            embeddings (list): embeddings
    &#34;&#34;&#34;

    if level == &#39;token&#39;:
        vocab = form_token_vocab(tokenizer, text)
        embeddings = form_token_embeddings(text, tokenizer, model)
    elif level == &#39;word&#39;:
        vocab = form_word_vocab(text)
        embeddings = form_word_embeddings(text, vocab, tokenizer, model, aggr_func)
    elif level == &#39;sentence&#39;:
        vocab = form_sentence_vocab(text)
        embeddings = form_sentence_embeddings(vocab, tokenizer, model, aggr_func)

    return vocab, embeddings</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="viz_embeddings.extract.Level"><code class="flex name class">
<span>class <span class="ident">Level</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Input text segmentation levels</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Level(Enum):
    &#34;&#34;&#34; Input text segmentation levels
    &#34;&#34;&#34;

    token = 1
    word = 2
    sentence = 3</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="viz_embeddings.extract.Level.sentence"><code class="name">var <span class="ident">sentence</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="viz_embeddings.extract.Level.token"><code class="name">var <span class="ident">token</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="viz_embeddings.extract.Level.word"><code class="name">var <span class="ident">word</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="viz_embeddings" href="index.html">viz_embeddings</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="viz_embeddings.extract.extract_embeddings" href="#viz_embeddings.extract.extract_embeddings">extract_embeddings</a></code></li>
<li><code><a title="viz_embeddings.extract.filter_vocabulary" href="#viz_embeddings.extract.filter_vocabulary">filter_vocabulary</a></code></li>
<li><code><a title="viz_embeddings.extract.form_sentence_embeddings" href="#viz_embeddings.extract.form_sentence_embeddings">form_sentence_embeddings</a></code></li>
<li><code><a title="viz_embeddings.extract.form_sentence_vocab" href="#viz_embeddings.extract.form_sentence_vocab">form_sentence_vocab</a></code></li>
<li><code><a title="viz_embeddings.extract.form_token_embeddings" href="#viz_embeddings.extract.form_token_embeddings">form_token_embeddings</a></code></li>
<li><code><a title="viz_embeddings.extract.form_token_vocab" href="#viz_embeddings.extract.form_token_vocab">form_token_vocab</a></code></li>
<li><code><a title="viz_embeddings.extract.form_word_embeddings" href="#viz_embeddings.extract.form_word_embeddings">form_word_embeddings</a></code></li>
<li><code><a title="viz_embeddings.extract.form_word_vocab" href="#viz_embeddings.extract.form_word_vocab">form_word_vocab</a></code></li>
<li><code><a title="viz_embeddings.extract.unique_vocabulary" href="#viz_embeddings.extract.unique_vocabulary">unique_vocabulary</a></code></li>
<li><code><a title="viz_embeddings.extract.vocabulary_and_embeddings" href="#viz_embeddings.extract.vocabulary_and_embeddings">vocabulary_and_embeddings</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="viz_embeddings.extract.Level" href="#viz_embeddings.extract.Level">Level</a></code></h4>
<ul class="">
<li><code><a title="viz_embeddings.extract.Level.sentence" href="#viz_embeddings.extract.Level.sentence">sentence</a></code></li>
<li><code><a title="viz_embeddings.extract.Level.token" href="#viz_embeddings.extract.Level.token">token</a></code></li>
<li><code><a title="viz_embeddings.extract.Level.word" href="#viz_embeddings.extract.Level.word">word</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>